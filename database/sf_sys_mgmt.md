**System Management Schema:  ** Used to make local copies of the SNOWFLAKE.ACCOUNT_USAGE and SNOWFLAKE.ORGANIZATION_USAGE views for better performance, selective data access since we don't give everyone access to the SNOWFLAKE shared database.

First, let's get in our management area:
```sql****
use database mydb;
use schema sys_mgmt;
```
Next, let's get the first table we want (local copy of QUERY_HISTORY).  I want a field called LOGDATE, that is just the date that the query ended.  It's just easier than always looking for a field to use and probably having to cast it to a date datatype.
```sql
CREATE TABLE QUERY_HISTORY_ARCHIVE
AS
SELECT
END_TIME::DATE AS LOGDATE
,*
FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY;
```
Now that we have our archive table, and it is populated, we need to refresh this once in a while.  Let's create a task.  I'm going to add in a little minus in my query so I don't bring in data I already have.  Nobody wants duplicate rows.
```sql
CREATE OR REPLACE TASK ARCHIVE_QUERYHIST_TASK
  WAREHOUSE = WH_XS
  SCHEDULE = 'USING CRON 30 0,3,6,12,15,18,21 * * * America/Chicago'
--  TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD HH24'
AS
INSERT INTO MYDB.SYS_MGMT.QUERY_HISTORY_ARCHIVE
SELECT * FROM
(SELECT 
 	END_TIME::DATE AS LOGDATE
	,QUERY_ID
	,QUERY_TEXT
	,DATABASE_ID
	,DATABASE_NAME
	,SCHEMA_ID
	,SCHEMA_NAME
	,QUERY_TYPE
	,SESSION_ID
	,USER_NAME
	,ROLE_NAME
	,WAREHOUSE_ID
	,WAREHOUSE_NAME
	,WAREHOUSE_SIZE
	,WAREHOUSE_TYPE
	,CLUSTER_NUMBER
	,QUERY_TAG
	,EXECUTION_STATUS
	,ERROR_CODE
	,ERROR_MESSAGE
	,START_TIME
	,END_TIME
	,TOTAL_ELAPSED_TIME
	,BYTES_SCANNED
	,PERCENTAGE_SCANNED_FROM_CACHE
	,BYTES_WRITTEN
	,BYTES_WRITTEN_TO_RESULT
	,BYTES_READ_FROM_RESULT
	,ROWS_PRODUCED
	,ROWS_INSERTED
	,ROWS_UPDATED
	,ROWS_DELETED
	,ROWS_UNLOADED
	,BYTES_DELETED
	,PARTITIONS_SCANNED
	,PARTITIONS_TOTAL
	,BYTES_SPILLED_TO_LOCAL_STORAGE
	,BYTES_SPILLED_TO_REMOTE_STORAGE
	,BYTES_SENT_OVER_THE_NETWORK
	,COMPILATION_TIME
	,EXECUTION_TIME
	,QUEUED_PROVISIONING_TIME
	,QUEUED_REPAIR_TIME
	,QUEUED_OVERLOAD_TIME
	,TRANSACTION_BLOCKED_TIME
	,OUTBOUND_DATA_TRANSFER_CLOUD
	,OUTBOUND_DATA_TRANSFER_REGION
	,OUTBOUND_DATA_TRANSFER_BYTES
	,INBOUND_DATA_TRANSFER_CLOUD
	,INBOUND_DATA_TRANSFER_REGION
	,INBOUND_DATA_TRANSFER_BYTES
	,LIST_EXTERNAL_FILES_TIME
	,CREDITS_USED_CLOUD_SERVICES
	,RELEASE_VERSION
	,EXTERNAL_FUNCTION_TOTAL_INVOCATIONS
	,EXTERNAL_FUNCTION_TOTAL_SENT_ROWS
	,EXTERNAL_FUNCTION_TOTAL_RECEIVED_ROWS
	,EXTERNAL_FUNCTION_TOTAL_SENT_BYTES
	,EXTERNAL_FUNCTION_TOTAL_RECEIVED_BYTES
	,QUERY_LOAD_PERCENT
	,IS_CLIENT_GENERATED_STATEMENT
	,QUERY_ACCELERATION_BYTES_SCANNED
	,QUERY_ACCELERATION_PARTITIONS_SCANNED
	,QUERY_ACCELERATION_UPPER_LIMIT_SCALE_FACTOR
 FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
 WHERE CAST(START_TIME AS DATE) >= CURRENT_DATE()-1
MINUS 
SELECT
	LOGDATE
	,QUERY_ID
	,QUERY_TEXT
	,DATABASE_ID
	,DATABASE_NAME
	,SCHEMA_ID
	,SCHEMA_NAME
	,QUERY_TYPE
	,SESSION_ID
	,USER_NAME
	,ROLE_NAME
	,WAREHOUSE_ID
	,WAREHOUSE_NAME
	,WAREHOUSE_SIZE
	,WAREHOUSE_TYPE
	,CLUSTER_NUMBER
	,QUERY_TAG
	,EXECUTION_STATUS
	,ERROR_CODE
	,ERROR_MESSAGE
	,START_TIME
	,END_TIME
	,TOTAL_ELAPSED_TIME
	,BYTES_SCANNED
	,PERCENTAGE_SCANNED_FROM_CACHE
	,BYTES_WRITTEN
	,BYTES_WRITTEN_TO_RESULT
	,BYTES_READ_FROM_RESULT
	,ROWS_PRODUCED
	,ROWS_INSERTED
	,ROWS_UPDATED
	,ROWS_DELETED
	,ROWS_UNLOADED
	,BYTES_DELETED
	,PARTITIONS_SCANNED
	,PARTITIONS_TOTAL
	,BYTES_SPILLED_TO_LOCAL_STORAGE
	,BYTES_SPILLED_TO_REMOTE_STORAGE
	,BYTES_SENT_OVER_THE_NETWORK
	,COMPILATION_TIME
	,EXECUTION_TIME
	,QUEUED_PROVISIONING_TIME
	,QUEUED_REPAIR_TIME
	,QUEUED_OVERLOAD_TIME
	,TRANSACTION_BLOCKED_TIME
	,OUTBOUND_DATA_TRANSFER_CLOUD
	,OUTBOUND_DATA_TRANSFER_REGION
	,OUTBOUND_DATA_TRANSFER_BYTES
	,INBOUND_DATA_TRANSFER_CLOUD
	,INBOUND_DATA_TRANSFER_REGION
	,INBOUND_DATA_TRANSFER_BYTES
	,LIST_EXTERNAL_FILES_TIME
	,CREDITS_USED_CLOUD_SERVICES
	,RELEASE_VERSION
	,EXTERNAL_FUNCTION_TOTAL_INVOCATIONS
	,EXTERNAL_FUNCTION_TOTAL_SENT_ROWS
	,EXTERNAL_FUNCTION_TOTAL_RECEIVED_ROWS
	,EXTERNAL_FUNCTION_TOTAL_SENT_BYTES
	,EXTERNAL_FUNCTION_TOTAL_RECEIVED_BYTES
	,QUERY_LOAD_PERCENT
	,IS_CLIENT_GENERATED_STATEMENT
	,QUERY_ACCELERATION_BYTES_SCANNED
	,QUERY_ACCELERATION_PARTITIONS_SCANNED
	,QUERY_ACCELERATION_UPPER_LIMIT_SCALE_FACTOR
 FROM MYDB.SYS_MGMT.QUERY_HISTORY_ARCHIVE
WHERE CAST(START_TIME AS DATE) >= CURRENT_DATE()-1) DELTA
;
```
Let's do some task maintenance.  First, everytime we create or update a task, we need to turn it on (i.e. RESUME).  Then after it runs in the schedule, let's make sure it ran successfully.
```sql
alter task ARCHIVE_QUERYHIST_TASK SUSPEND;
alter task ARCHIVE_QUERYHIST_TASK RESUME;

SELECT * FROM TABLE(information_schema.task_history(
    scheduled_time_range_start=>dateadd('day',-6,current_timestamp()),
    result_limit => 20,
    task_name=>'ARCHIVE_QUERYHIST_TASK'));
```

Finally, we don't normally send users to the tables, we give them a view to query.  
```sql
create view MYDB.SYS_MGMT_V.QUERY_HISTORY_ARCHIVE(
	LOGDATE,
	QUERY_ID,
	QUERY_TEXT,
	DATABASE_ID,
	DATABASE_NAME,
	SCHEMA_ID,
	SCHEMA_NAME,
	QUERY_TYPE,
	SESSION_ID,
	USER_NAME,
	ROLE_NAME,
	WAREHOUSE_ID,
	WAREHOUSE_NAME,
	WAREHOUSE_SIZE,
	WAREHOUSE_TYPE,
	CLUSTER_NUMBER,
	QUERY_TAG,
	EXECUTION_STATUS,
	ERROR_CODE,
	ERROR_MESSAGE,
	START_TIME,
	END_TIME,
	TOTAL_ELAPSED_TIME,
	BYTES_SCANNED,
	PERCENTAGE_SCANNED_FROM_CACHE,
	BYTES_WRITTEN,
	BYTES_WRITTEN_TO_RESULT,
	BYTES_READ_FROM_RESULT,
	ROWS_PRODUCED,
	ROWS_INSERTED,
	ROWS_UPDATED,
	ROWS_DELETED,
	ROWS_UNLOADED,
	BYTES_DELETED,
	PARTITIONS_SCANNED,
	PARTITIONS_TOTAL,
	BYTES_SPILLED_TO_LOCAL_STORAGE,
	BYTES_SPILLED_TO_REMOTE_STORAGE,
	BYTES_SENT_OVER_THE_NETWORK,
	COMPILATION_TIME,
	EXECUTION_TIME,
	QUEUED_PROVISIONING_TIME,
	QUEUED_REPAIR_TIME,
	QUEUED_OVERLOAD_TIME,
	TRANSACTION_BLOCKED_TIME,
	OUTBOUND_DATA_TRANSFER_CLOUD,
	OUTBOUND_DATA_TRANSFER_REGION,
	OUTBOUND_DATA_TRANSFER_BYTES,
	INBOUND_DATA_TRANSFER_CLOUD,
	INBOUND_DATA_TRANSFER_REGION,
	INBOUND_DATA_TRANSFER_BYTES,
	LIST_EXTERNAL_FILES_TIME,
	CREDITS_USED_CLOUD_SERVICES,
	RELEASE_VERSION,
	EXTERNAL_FUNCTION_TOTAL_INVOCATIONS,
	EXTERNAL_FUNCTION_TOTAL_SENT_ROWS,
	EXTERNAL_FUNCTION_TOTAL_RECEIVED_ROWS,
	EXTERNAL_FUNCTION_TOTAL_SENT_BYTES,
	EXTERNAL_FUNCTION_TOTAL_RECEIVED_BYTES,
	QUERY_LOAD_PERCENT,
	IS_CLIENT_GENERATED_STATEMENT,
	QUERY_ACCELERATION_BYTES_SCANNED,
	QUERY_ACCELERATION_PARTITIONS_SCANNED,
	QUERY_ACCELERATION_UPPER_LIMIT_SCALE_FACTOR	
) AS
(
  SELECT LOGDATE,
	QUERY_ID,
	QUERY_TEXT,
	DATABASE_ID,
	DATABASE_NAME,
	SCHEMA_ID,
	SCHEMA_NAME,
	QUERY_TYPE,
	SESSION_ID,
	USER_NAME,
	ROLE_NAME,
	WAREHOUSE_ID,
	WAREHOUSE_NAME,
	WAREHOUSE_SIZE,
	WAREHOUSE_TYPE,
	CLUSTER_NUMBER,
	QUERY_TAG,
	EXECUTION_STATUS,
	ERROR_CODE,
	ERROR_MESSAGE,
	START_TIME,
	END_TIME,
	TOTAL_ELAPSED_TIME,
	BYTES_SCANNED,
	PERCENTAGE_SCANNED_FROM_CACHE,
	BYTES_WRITTEN,
	BYTES_WRITTEN_TO_RESULT,
	BYTES_READ_FROM_RESULT,
	ROWS_PRODUCED,
	ROWS_INSERTED,
	ROWS_UPDATED,
	ROWS_DELETED,
	ROWS_UNLOADED,
	BYTES_DELETED,
	PARTITIONS_SCANNED,
	PARTITIONS_TOTAL,
	BYTES_SPILLED_TO_LOCAL_STORAGE,
	BYTES_SPILLED_TO_REMOTE_STORAGE,
	BYTES_SENT_OVER_THE_NETWORK,
	COMPILATION_TIME,
	EXECUTION_TIME,
	QUEUED_PROVISIONING_TIME,
	QUEUED_REPAIR_TIME,
	QUEUED_OVERLOAD_TIME,
	TRANSACTION_BLOCKED_TIME,
	OUTBOUND_DATA_TRANSFER_CLOUD,
	OUTBOUND_DATA_TRANSFER_REGION,
	OUTBOUND_DATA_TRANSFER_BYTES,
	INBOUND_DATA_TRANSFER_CLOUD,
	INBOUND_DATA_TRANSFER_REGION,
	INBOUND_DATA_TRANSFER_BYTES,
	LIST_EXTERNAL_FILES_TIME,
	CREDITS_USED_CLOUD_SERVICES,
	RELEASE_VERSION,
	EXTERNAL_FUNCTION_TOTAL_INVOCATIONS,
	EXTERNAL_FUNCTION_TOTAL_SENT_ROWS,
	EXTERNAL_FUNCTION_TOTAL_RECEIVED_ROWS,
	EXTERNAL_FUNCTION_TOTAL_SENT_BYTES,
	EXTERNAL_FUNCTION_TOTAL_RECEIVED_BYTES,
	QUERY_LOAD_PERCENT,
	IS_CLIENT_GENERATED_STATEMENT,
	QUERY_ACCELERATION_BYTES_SCANNED,
	QUERY_ACCELERATION_PARTITIONS_SCANNED,
	QUERY_ACCELERATION_UPPER_LIMIT_SCALE_FACTOR	
FROM SFDB_PROD.SYS_MGMT.QUERY_HISTORY_ARCHIVE
)
;
```
Now, do the same thing for all of the other SNOWFLAKE.ACCOUNT_USAGE tables you want a local copy of.

If your company has multiple Snowflake accounts set up, we can still use this process to dump data to a single repo for some tables.  Specifically, the WAREHOUSE_METERING_HISTORY might be nice to have in one place for all warehouses for all Snowflake accounts in your company.

Let's create the table, task, etc.  Really, it is all the same, we are just sourcing from SNOWFLAKE.ORGANIZATION_USAGE instead of ACCOUNT_USAGE.
```sql
CREATE TABLE MYDB.SYS_MGMT.ORG_WAREHOUSE_METERING_HISTORY_ARCHIVE 
AS 
select END_TIME::DATE AS LOGDATE
,* 
from SNOWFLAKE.ORGANIZATION_USAGE.WAREHOUSE_METERING_HISTORY;

CREATE OR REPLACE TASK ARCHIVE_ORG_WHMETERINGHIST_TASK
  WAREHOUSE = WH_XS
  SCHEDULE = 'USING CRON 30 0,12 * * * America/Chicago'
--  TIMESTAMP_INPUT_FORMAT = 'YYYY-MM-DD HH24'
AS
INSERT INTO SFDB_PROD.SYS_MGMT.ORG_WAREHOUSE_METERING_HISTORY_ARCHIVE
SELECT * FROM
(SELECT 
 END_TIME::DATE AS LOGDATE
,ORGANIZATION_NAME
,ACCOUNT_NAME
,REGION
,SERVICE_TYPE
,START_TIME
,END_TIME
,WAREHOUSE_ID
,WAREHOUSE_NAME
,CREDITS_USED
,CREDITS_USED_COMPUTE
,CREDITS_USED_CLOUD_SERVICES
 ,ACCOUNT_LOCATOR
 FROM SNOWFLAKE.ORGANIZATION_USAGE.WAREHOUSE_METERING_HISTORY
 WHERE CAST(END_TIME AS DATE) >= CURRENT_DATE()-1
MINUS 
SELECT
LOGDATE
,ORGANIZATION_NAME
,ACCOUNT_NAME
,REGION
,SERVICE_TYPE
,START_TIME
,END_TIME
,WAREHOUSE_ID
,WAREHOUSE_NAME
,CREDITS_USED
,CREDITS_USED_COMPUTE
,CREDITS_USED_CLOUD_SERVICES
 ,ACCOUNT_LOCATOR
 FROM SFDB_PROD.SYS_MGMT.ORG_WAREHOUSE_METERING_HISTORY_ARCHIVE
WHERE CAST(END_TIME AS DATE) >= CURRENT_DATE()-1) DELTA
;
alter task ARCHIVE_ORG_WHMETERINGHIST_TASK resume;
```
